# 孪生网络和三胞胎与在线配对/三胞胎挖掘



孪生和三胞胎网络对于学习从图像到紧凑的欧几里得空间的映射非常有用，其中距离对应于相似度的测量[2]。以这种方式训练的嵌入可以作为特征向量用于分类或少数镜头学习任务。


# 实例 - MNIST


## Baseline - 用softmax分类

我们增加了一个全连接层的类数，用softmax和交叉熵训练网络进行分类。网络训练的准确率约为99%。我们从倒数第二层提取2维嵌入。

Train set:

![enter image description here](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/mnist_softmax_train.png)

Test set:

![enter image description here](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/mnist_softmax_test.png)

虽然嵌入看起来是可分离的（这也是我们训练它们的目的），但它们并不具备良好的度量属性。它们可能不是作为新类的描述符的最佳选择。

## 孪生神经网络

现在我们将训练一个连体网络，它将一对图像进行训练嵌入，如果它们来自同一类，则它们之间的距离最小化，如果它们代表不同的类，则大于某个余量值。
我们将最小化一个contrastive loss损失函数[1]。

![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/contrastive_loss.png)

*
SiameseMNIST*类随机采样正负对，然后将其反馈给Siamese网络。

经过20个纪元的训练，这里是我们得到的训练集的嵌入。

![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/mnist_siamese_train.png)

Test set:

![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/mnist_siamese_test.png)

学到的嵌入在类内聚类效果更好。

## 三胞胎网络

我们将训练一个三联网络，它需要一个Anchor、一个Positive（与Anchor同级）和Negative（与Anchor不同级）的例子。目标是学习嵌入，使锚与正例的关系比与负例的关系更接近，并有一定的边际值。

![alt text](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/anchor_negative_positive.png "Source: FaceNet")
Source: *Schroff, Florian, Dmitry Kalenichenko, and James Philbin. [Facenet: A unified embedding for face recognition and clustering.](https://arxiv.org/abs/1503.03832) CVPR 2015.*

**Triplet loss**:   ![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/triplet_loss.png)

*
TripletMNIST*类为每一个可能的锚取样一个正例和负例。

经过20个epochs的训练，这里是我们得到的训练集的嵌入。

![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/mnist_triplet_train.png)

Test set:

![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/mnist_triplet_test.png)

学习到的嵌入在类内并不像在siamese网络的情况下那样相互接近，但这不是我们优化它们的目的。我们希望嵌入与同类的其他嵌入比其他类的更接近，我们可以看到这就是训练的方向。

## Online pair/triplet selection - negative mining

孪生和三胞胎网络有几个问题：
1. 可能的对子/三联**的数量随着实例数量的增加而**二次/三次**。要处理所有的例子是不可行的，训练收敛得很慢。
2. 我们*随机地生成对/三联体。随着训练的不断进行，越来越多的对/三胞胎是**容易处理的（它们的损失值非常小，甚至是0），*阻止网络的训练*。我们需要为网络提供**难的例子**。
3. 输入到网络中的每幅图像只用于计算一对/三胞胎的对比度/三胞胎损失。计算有些浪费；一旦计算出嵌入，就可以重复用于许多对/三联体。

为了有效地处理这些问题，我们将像分类一样用标准的mini-batch馈送网络。损失函数将负责mini-batch内硬对和三胞胎的选择。如果我们用每10个类16张图片来喂养网络，我们可以处理最多159/\*160/2=12720对和10/\*16/15/2/\*(9/\*16)=172800个三胞胎，而之前的实现中只有80对和53个三胞胎。

通常情况下，在一个小批量内处理所有可能的对子或三胞胎并不是最好的主意。我们可以在[2]和[3]中找到一些关于如何选择三胞胎的策略。

### 在线选择配对


我们将用mini-batch喂养一个网络，就像我们对分类网络所做的那样。这次我们将使用一个特殊的BatchSampler，它将对每个类中的*n_classes*和*n_samples*进行采样，从而得到大小为*n_classes/*n_samples*的迷你批。

对于每个迷你批次，将使用提供的标签选择正负对。

MNIST是一个相当简单的数据集，从随机选取的对子的嵌入已经相当好了，我们看不到这里有什么改进。

**Train embeddings:**

![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/mnist_ocl_train.png)

**Test embeddings:**

![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/mnist_ocl_test.png)

### 在线三胞胎选择

我们将像在线配对选择一样，用迷你批次喂养一个网络。在给定标签和预测嵌入的情况下，我们可以使用一些策略来进行三胞胎选择:

- 所有可能的三胞胎(可能太多)
- 每个正数对的最难负数(将导致每个锚的负数相同)
- 每个正对的随机硬负值(只考虑三倍体损失值为正的三倍体)
- 每对正数的半硬负数（类似于[2]）。


三重选择的策略必须谨慎选择。一个不好的策略可能会导致训练效率低下，甚至更糟糕的是，会导致模型坍塌（所有的嵌入最终都有相同的值）。

下面是我们对每个正对进行随机硬否定的结果。

**Training set:**

![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/mnist_otl_train.png)

**Test set:**

![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/mnist_otl_test.png)

# FashionMNIST

Similar experiments were conducted for [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) 数据集也进行了类似的实验，在线负向挖掘的优势略显明显。使用了完全相同的网络架构，只有二维嵌入，这对于学习良好的嵌入可能不够复杂。

更复杂的数据集，更多的classses数量，应该会从在线挖掘中获益更多。


## Baseline - classification

![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/fmnist_softmax_test.png)

## Siamese vs online contrastive loss with negative mining

Siamese network with randomly selected pairs

![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/fmnist_comp_cl.png)

Online contrastive loss with negative mining

![](images/fmnist_comp_ocl.png)

## Triplet vs online triplet loss with negative mining

Triplet network with random triplets

![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/fmnist_comp_tl.png)

Online triplet loss with negative mining

![](https://raw.githubusercontent.com/adambielski/siamese-triplet/master/images/fmnist_comp_otl.png)

# TODO

- [ ] Optimize triplet selection
- [ ] Evaluate with a metric that is comparable between approaches
- [ ] Evaluate in one-shot setting when classes from test set are not in train set
- [ ] Show online triplet selection example on more difficult datasets

# References

[1] Raia Hadsell, Sumit Chopra, Yann LeCun, [Dimensionality reduction by learning an invariant mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf), CVPR 2006

[2] Schroff, Florian, Dmitry Kalenichenko, and James Philbin. [Facenet: A unified embedding for face recognition and clustering.](https://arxiv.org/abs/1503.03832) CVPR 2015

[3] Alexander Hermans, Lucas Beyer, Bastian Leibe, [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/pdf/1703.07737), 2017

[4] Brandon Amos, Bartosz Ludwiczuk, Mahadev Satyanarayanan, [OpenFace: A general-purpose face recognition library with mobile applications](http://reports-archive.adm.cs.cmu.edu/anon/2016/CMU-CS-16-118.pdf), 2016

[5] Yi Sun, Xiaogang Wang, Xiaoou Tang, [Deep Learning Face Representation by Joint Identification-Verification](http://papers.nips.cc/paper/5416-deep-learning-face-representation-by-joint-identification-verification), NIPS 2014
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTgwMDI1MjVdfQ==
-->